{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Better to fix the seed in the beginning:\n",
    "seed = 666\n",
    "np.random.seed(seed)\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "from keras import backend \n",
    "from keras.models import Sequential\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility functions (data preprocessing and KFold cross-validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Scale and center features, transform labels into a one-hot encoding vector:\n",
    "def preprocess_data(X, y):\n",
    "### TO DO ###\n",
    "    X_out = scale(X)\n",
    "    y_out = to_categorical(y)\n",
    "    return X_out, y_out\n",
    "\n",
    "### Training history plot function: (this function is finished, nothing to add !)\n",
    "def print_training_history(training_history, fig_idx):\n",
    "    epoch_absciss = range(1, len(training_history.history['loss'])+1)\n",
    "    plt.figure(fig_idx, figsize=(10, 5))\n",
    "    plt.suptitle(\"MLP model assessment\")\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epoch_absciss, training_history.history['loss'])\n",
    "    plt.plot(epoch_absciss, training_history.history['val_loss'])\n",
    "    plt.title(\"Train/Validation loss\")\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.legend(['Train loss', 'Validation loss'], loc='best')\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epoch_absciss, training_history.history['accuracy'])\n",
    "    plt.plot(epoch_absciss, training_history.history['val_accuracy'])\n",
    "    plt.title(\"Train/Validation accuracy\")\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.legend(['Train accuracy', 'Validation accuracy'], loc='best')\n",
    "    plt.show()\n",
    "    \n",
    "### Select a MLP model on a list of hyper-parameters instances, via Kfold cross-validation:\n",
    "def KFold_model_selection(X, y, fixed_hyper_parameters, hyper_parameters_instances, num_folds, seed):\n",
    "### TO DO ###\n",
    "    def KFold_split(X, Y, num_folds, seed):\n",
    "        KFold_splitter = KFold(n_splits=num_folds, shuffle=True, random_state=seed)\n",
    "        X_train_folds = []\n",
    "        X_val_folds = []\n",
    "        Y_train_folds = []\n",
    "        Y_val_folds = []\n",
    "        for (kth_fold_train_idxs, kth_fold_val_idxs) in KFold_splitter.split(X, Y):\n",
    "            X_train_folds.append(X[kth_fold_train_idxs])\n",
    "            X_val_folds.append(X[kth_fold_val_idxs])\n",
    "            Y_train_folds.append(Y[kth_fold_train_idxs])\n",
    "            Y_val_folds.append(Y[kth_fold_val_idxs])\n",
    "        return X_train_folds, X_val_folds, Y_train_folds, Y_val_folds\n",
    "    \n",
    "    X_train_val, X_test, Y_train_val, Y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n",
    "    X_train_folds, X_val_folds, Y_train_folds, Y_val_folds = KFold_split(X_train_val, Y_train_val, num_folds, seed)\n",
    "    mean_val_MSEs = []\n",
    "    for hyper_parameters_instance in hyper_parameters_instances:\n",
    "        print(\"\\nNow preprocessing hyper-parameter instance\", hyper_parameters_instance)\n",
    "        mean_val_MSE = perform_KFold_CV(X_train_folds, X_val_folds, Y_train_folds, Y_val_folds,\n",
    "                                       fixed_hyper_parameters, \n",
    "                                       hyper_parameters_instance)\n",
    "        print(\"Mean validation MSE:\", mean_val_MSE)\n",
    "        mean_val_MSEs.append(mean_val_MSE)\n",
    "    best_instance_idx = mean_val_MSEs.index(min(mean_val_MSEs))\n",
    "    best_hyper_parameters_instance = hyper_parameters_instances[best_instance_idx]\n",
    "    print(\"\\n\\nBest hyper-parameter instance:\", best_hyper_parameters_instance)\n",
    "    best_model_test_MSE = assess_MLP(X_train_val, X_test, Y_train_val, Y_test,\n",
    "                                       fixed_hyper_parameters,\n",
    "                                       hyper_parameters_instances[best_instance_idx])\n",
    "    print(\"Test MSE:\", best_model_test_MSE)\n",
    "                                       \n",
    "    return\n",
    "\n",
    "### KFold cross-validation of a MLP model with given hyper-parameters:\n",
    "def perform_KFold_CV(X_train_folds, X_val_folds, Y_train_folds, Y_val_folds, fixed_hyper_parameters, hyper_parameters_instance):\n",
    "### TO DO ###\n",
    "    val_fold_MSEs = []\n",
    "    # For each fold, assess a surrogate model with fixed hyper-parameters:\n",
    "    cmpt = 0\n",
    "    for X_train_fold, X_val_fold, Y_train_fold, Y_val_fold in zip(X_train_folds, X_val_folds, Y_train_folds, Y_val_folds):\n",
    "        val_fold_MSE = assess_MLP(X_train_fold, X_val_fold, Y_train_fold, Y_val_fold, fixed_hyper_parameters, hyper_parameters_instance)\n",
    "        cmpt += 1\n",
    "#         print(\"Surrogate model\", str(cmpt) + \"/\" + str(len(X_val_folds)), \"validation MSE:\", val_fold_MSE)\n",
    "        val_fold_MSEs.append(val_fold_MSE)\n",
    "    # Compute the mean validation MSE between all the folds:\n",
    "    mean_val_MSE = np.mean(val_fold_MSE)\n",
    "    return mean_val_MSE\n",
    "### Fit and evaluate a MLP model with given hyper-parameters:\n",
    "def assess_MLP(X_train, X_test, y_train, y_test, fixed_hyper_parameters, hyper_parameters_instance, verbose=False):\n",
    "### TO DO ###\n",
    "    in_shape = X_train.shape[1]\n",
    "    num_y_classes = y_train.shape[1]\n",
    "    myMLP = build_MLP(in_shape, num_y_classes, hyper_parameters_instance)\n",
    "    myMLP.fit(X_train, y_train,\\\n",
    "              batch_size=fixed_hyper_parameters[\"train batch size\"],\\\n",
    "              epochs=fixed_hyper_parameters[\"epochs\"])\n",
    "    mytest_loss, mytest_accuracy = myMLP.evaluate(X_test, y_test, fixed_hyper_parameters[\"train batch size\"])\n",
    "    return mytest_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP (multi-layer perceptron) builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Build a simple fully-connected MLP with SGD model:\n",
    "def build_MLP(input_shape, num_classes, hyper_parameters_instance=None): #add hyper parameters here\n",
    "    MLP = Sequential()\n",
    "    # Hidden layers (fully connected/dense):\n",
    "    if hyper_parameters_instance==None:\n",
    "        MLP.add(Dense(10, activation='relu'))\n",
    "    else:\n",
    "        if hyper_parameters_instance[\"HiddenLayerActivationRelu\"]==True:\n",
    "            MLP.add(Dense(10, activation='relu'))\n",
    "            if hyper_parameters_instance[\"Flag\"]==True:\n",
    "                MLP.add(Dense(10, activation='relu'))\n",
    "        else:\n",
    "            MLP.add(Dense(10, activation='sigmoid'))\n",
    "            if hyper_parameters_instance[\"Flag\"]==True:\n",
    "                MLP.add(Dense(10, activation='sigmoid'))\n",
    "    # Output layer (fully-connected/dense):\n",
    "    MLP.add(Dense(units=num_classes, activation='softmax'))\n",
    "#     sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    MLP.compile(loss='categorical_crossentropy', optimizer=\"SGD\", metrics=['accuracy'])\n",
    "    return MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and preprocess the Wine dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Wine dataset:\n",
    "X = load_wine().data\n",
    "y = load_wine().target\n",
    "# Get the shape of the individual feature vectors in the dataset:\n",
    "input_shape = X.shape[1]\n",
    "# Get the number of classes:\n",
    "num_classes = (np.unique(y)).shape[0]\n",
    "# Preprocess data: (implement the preprocess_data function)\n",
    "X, y = preprocess_data(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train, validate and evaluate a MLP model, and plot the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 99 samples, validate on 25 samples\n",
      "Epoch 1/20\n",
      "99/99 [==============================] - 0s 594us/step - loss: 1.0801 - accuracy: 0.4141 - val_loss: 0.8081 - val_accuracy: 0.7600\n",
      "Epoch 2/20\n",
      "99/99 [==============================] - 0s 91us/step - loss: 1.0000 - accuracy: 0.4646 - val_loss: 0.7589 - val_accuracy: 0.8000\n",
      "Epoch 3/20\n",
      "99/99 [==============================] - 0s 81us/step - loss: 0.9341 - accuracy: 0.5556 - val_loss: 0.7150 - val_accuracy: 0.8000\n",
      "Epoch 4/20\n",
      "99/99 [==============================] - 0s 90us/step - loss: 0.8765 - accuracy: 0.6061 - val_loss: 0.6743 - val_accuracy: 0.8000\n",
      "Epoch 5/20\n",
      "99/99 [==============================] - 0s 100us/step - loss: 0.8231 - accuracy: 0.6364 - val_loss: 0.6372 - val_accuracy: 0.8000\n",
      "Epoch 6/20\n",
      "99/99 [==============================] - 0s 92us/step - loss: 0.7735 - accuracy: 0.6667 - val_loss: 0.6060 - val_accuracy: 0.8000\n",
      "Epoch 7/20\n",
      "99/99 [==============================] - 0s 90us/step - loss: 0.7332 - accuracy: 0.7273 - val_loss: 0.5766 - val_accuracy: 0.8000\n",
      "Epoch 8/20\n",
      "99/99 [==============================] - 0s 91us/step - loss: 0.6946 - accuracy: 0.7778 - val_loss: 0.5496 - val_accuracy: 0.8000\n",
      "Epoch 9/20\n",
      "99/99 [==============================] - 0s 91us/step - loss: 0.6590 - accuracy: 0.7879 - val_loss: 0.5237 - val_accuracy: 0.8000\n",
      "Epoch 10/20\n",
      "99/99 [==============================] - 0s 110us/step - loss: 0.6262 - accuracy: 0.8283 - val_loss: 0.5013 - val_accuracy: 0.8000\n",
      "Epoch 11/20\n",
      "99/99 [==============================] - 0s 111us/step - loss: 0.5974 - accuracy: 0.8788 - val_loss: 0.4800 - val_accuracy: 0.8800\n",
      "Epoch 12/20\n",
      "99/99 [==============================] - 0s 101us/step - loss: 0.5700 - accuracy: 0.8889 - val_loss: 0.4605 - val_accuracy: 0.8800\n",
      "Epoch 13/20\n",
      "99/99 [==============================] - 0s 100us/step - loss: 0.5446 - accuracy: 0.9091 - val_loss: 0.4421 - val_accuracy: 0.8800\n",
      "Epoch 14/20\n",
      "99/99 [==============================] - 0s 91us/step - loss: 0.5211 - accuracy: 0.9192 - val_loss: 0.4263 - val_accuracy: 0.8800\n",
      "Epoch 15/20\n",
      "99/99 [==============================] - 0s 101us/step - loss: 0.5005 - accuracy: 0.9192 - val_loss: 0.4110 - val_accuracy: 0.9200\n",
      "Epoch 16/20\n",
      "99/99 [==============================] - 0s 101us/step - loss: 0.4798 - accuracy: 0.9192 - val_loss: 0.3959 - val_accuracy: 0.9200\n",
      "Epoch 17/20\n",
      "99/99 [==============================] - 0s 91us/step - loss: 0.4600 - accuracy: 0.9192 - val_loss: 0.3817 - val_accuracy: 0.9200\n",
      "Epoch 18/20\n",
      "99/99 [==============================] - 0s 101us/step - loss: 0.4419 - accuracy: 0.9192 - val_loss: 0.3684 - val_accuracy: 0.9200\n",
      "Epoch 19/20\n",
      "99/99 [==============================] - 0s 110us/step - loss: 0.4245 - accuracy: 0.9192 - val_loss: 0.3567 - val_accuracy: 0.9200\n",
      "Epoch 20/20\n",
      "99/99 [==============================] - 0s 103us/step - loss: 0.4090 - accuracy: 0.9192 - val_loss: 0.3449 - val_accuracy: 0.9200\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 10)                140       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 33        \n",
      "=================================================================\n",
      "Total params: 173\n",
      "Trainable params: 173\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "54/54 [==============================] - 0s 55us/step\n",
      "Test loss: 0.38749685883522034\n",
      "Test accuracy: 0.9814814925193787\n"
     ]
    }
   ],
   "source": [
    "# Number of epochs:\n",
    "num_epochs = 20\n",
    "# Train batch size:\n",
    "train_batch_size = 16\n",
    "# Split data into train/val/test sets:\n",
    "X_train_val, X_test, Y_train_val, Y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n",
    "\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train_val, Y_train_val, test_size = 0.2, random_state = 0)\n",
    "\n",
    "# Load an MLP:\n",
    "model = build_MLP(input_shape, num_classes)\n",
    "# print(model.summary())\n",
    "# Train and validate MLP, store the training history in a variable:\n",
    "training_history = model.fit(X_train, Y_train,\n",
    "                             batch_size = train_batch_size, epochs = num_epochs,\n",
    "                             validation_data = [X_val , Y_val])\n",
    "model.summary()\n",
    "# Evaluate the model:\n",
    "test_loss, test_accuracy = model.evaluate(X_test, Y_test, train_batch_size)\n",
    "print(\"Test loss:\", test_loss)\n",
    "print(\"Test accuracy:\", test_accuracy)\n",
    "# Plot training history:\n",
    "# print_training_history(training_history, fig_idx=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model selection of our MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Now preprocessing hyper-parameter instance {'Flag': True, 'HiddenLayerActivationRelu': True}\n",
      "Epoch 1/5\n",
      "99/99 [==============================] - 0s 525us/step - loss: 1.1335 - accuracy: 0.3232\n",
      "Epoch 2/5\n",
      "99/99 [==============================] - 0s 60us/step - loss: 1.0959 - accuracy: 0.4242\n",
      "Epoch 3/5\n",
      "99/99 [==============================] - 0s 81us/step - loss: 1.0611 - accuracy: 0.4646\n",
      "Epoch 4/5\n",
      "99/99 [==============================] - 0s 70us/step - loss: 1.0278 - accuracy: 0.5354\n",
      "Epoch 5/5\n",
      "99/99 [==============================] - 0s 80us/step - loss: 0.9964 - accuracy: 0.5657\n",
      "25/25 [==============================] - 0s 640us/step\n",
      "Epoch 1/5\n",
      "99/99 [==============================] - 0s 465us/step - loss: 1.2935 - accuracy: 0.3737\n",
      "Epoch 2/5\n",
      "99/99 [==============================] - 0s 75us/step - loss: 1.2173 - accuracy: 0.3737\n",
      "Epoch 3/5\n",
      "99/99 [==============================] - 0s 81us/step - loss: 1.1460 - accuracy: 0.3535\n",
      "Epoch 4/5\n",
      "99/99 [==============================] - 0s 91us/step - loss: 1.0842 - accuracy: 0.3434\n",
      "Epoch 5/5\n",
      "99/99 [==============================] - 0s 81us/step - loss: 1.0268 - accuracy: 0.3636\n",
      "25/25 [==============================] - 0s 637us/step\n",
      "Epoch 1/5\n",
      "99/99 [==============================] - 0s 499us/step - loss: 1.3209 - accuracy: 0.2020\n",
      "Epoch 2/5\n",
      "99/99 [==============================] - 0s 81us/step - loss: 1.2384 - accuracy: 0.2020\n",
      "Epoch 3/5\n",
      "99/99 [==============================] - 0s 71us/step - loss: 1.1746 - accuracy: 0.2727\n",
      "Epoch 4/5\n",
      "99/99 [==============================] - 0s 81us/step - loss: 1.1287 - accuracy: 0.2727\n",
      "Epoch 5/5\n",
      "99/99 [==============================] - 0s 90us/step - loss: 1.0901 - accuracy: 0.3333\n",
      "25/25 [==============================] - 0s 637us/step\n",
      "Epoch 1/5\n",
      "99/99 [==============================] - 0s 488us/step - loss: 1.0993 - accuracy: 0.3333\n",
      "Epoch 2/5\n",
      "99/99 [==============================] - 0s 71us/step - loss: 1.0279 - accuracy: 0.4848\n",
      "Epoch 3/5\n",
      "99/99 [==============================] - 0s 60us/step - loss: 0.9782 - accuracy: 0.6263\n",
      "Epoch 4/5\n",
      "99/99 [==============================] - 0s 71us/step - loss: 0.9322 - accuracy: 0.6970\n",
      "Epoch 5/5\n",
      "99/99 [==============================] - 0s 80us/step - loss: 0.8877 - accuracy: 0.7273\n",
      "25/25 [==============================] - 0s 639us/step\n",
      "Epoch 1/5\n",
      "100/100 [==============================] - 0s 480us/step - loss: 1.2854 - accuracy: 0.1000\n",
      "Epoch 2/5\n",
      "100/100 [==============================] - 0s 60us/step - loss: 1.2288 - accuracy: 0.1400\n",
      "Epoch 3/5\n",
      "100/100 [==============================] - 0s 70us/step - loss: 1.1825 - accuracy: 0.1700\n",
      "Epoch 4/5\n",
      "100/100 [==============================] - 0s 70us/step - loss: 1.1448 - accuracy: 0.2000\n",
      "Epoch 5/5\n",
      "100/100 [==============================] - 0s 79us/step - loss: 1.1106 - accuracy: 0.2300\n",
      "24/24 [==============================] - 0s 621us/step\n",
      "Mean validation MSE: 1.0684965054194133\n",
      "\n",
      "Now preprocessing hyper-parameter instance {'Flag': True, 'HiddenLayerActivationRelu': False}\n",
      "Epoch 1/5\n",
      "99/99 [==============================] - 0s 453us/step - loss: 1.1183 - accuracy: 0.4040\n",
      "Epoch 2/5\n",
      "99/99 [==============================] - 0s 76us/step - loss: 1.1093 - accuracy: 0.4040\n",
      "Epoch 3/5\n",
      "99/99 [==============================] - 0s 70us/step - loss: 1.1021 - accuracy: 0.4040\n",
      "Epoch 4/5\n",
      "99/99 [==============================] - 0s 83us/step - loss: 1.0950 - accuracy: 0.4040\n",
      "Epoch 5/5\n",
      "99/99 [==============================] - 0s 81us/step - loss: 1.0881 - accuracy: 0.4040\n",
      "25/25 [==============================] - 0s 798us/step\n",
      "Epoch 1/5\n",
      "99/99 [==============================] - 0s 463us/step - loss: 1.1907 - accuracy: 0.4242\n",
      "Epoch 2/5\n",
      "99/99 [==============================] - 0s 80us/step - loss: 1.1770 - accuracy: 0.3737\n",
      "Epoch 3/5\n",
      "99/99 [==============================] - 0s 71us/step - loss: 1.1673 - accuracy: 0.4040\n",
      "Epoch 4/5\n",
      "99/99 [==============================] - 0s 81us/step - loss: 1.1543 - accuracy: 0.4242\n",
      "Epoch 5/5\n",
      "99/99 [==============================] - 0s 76us/step - loss: 1.1409 - accuracy: 0.4444\n",
      "25/25 [==============================] - 0s 640us/step\n",
      "Epoch 1/5\n",
      "99/99 [==============================] - 0s 556us/step - loss: 1.2032 - accuracy: 0.4242\n",
      "Epoch 2/5\n",
      "99/99 [==============================] - 0s 81us/step - loss: 1.1823 - accuracy: 0.4242\n",
      "Epoch 3/5\n",
      "99/99 [==============================] - 0s 90us/step - loss: 1.1700 - accuracy: 0.4242\n",
      "Epoch 4/5\n",
      "99/99 [==============================] - 0s 91us/step - loss: 1.1558 - accuracy: 0.4242\n",
      "Epoch 5/5\n",
      "99/99 [==============================] - 0s 81us/step - loss: 1.1447 - accuracy: 0.4242\n",
      "25/25 [==============================] - 0s 640us/step\n",
      "Epoch 1/5\n",
      "99/99 [==============================] - 0s 505us/step - loss: 1.1748 - accuracy: 0.2828\n",
      "Epoch 2/5\n",
      "99/99 [==============================] - 0s 80us/step - loss: 1.1560 - accuracy: 0.2828\n",
      "Epoch 3/5\n",
      "99/99 [==============================] - 0s 91us/step - loss: 1.1380 - accuracy: 0.2828\n",
      "Epoch 4/5\n",
      "99/99 [==============================] - 0s 71us/step - loss: 1.1280 - accuracy: 0.2828\n",
      "Epoch 5/5\n",
      "99/99 [==============================] - 0s 82us/step - loss: 1.1177 - accuracy: 0.2828\n",
      "25/25 [==============================] - 0s 780us/step\n",
      "Epoch 1/5\n",
      "100/100 [==============================] - 0s 464us/step - loss: 1.0787 - accuracy: 0.4300\n",
      "Epoch 2/5\n",
      "100/100 [==============================] - 0s 59us/step - loss: 1.0784 - accuracy: 0.4300\n",
      "Epoch 3/5\n",
      "100/100 [==============================] - 0s 79us/step - loss: 1.0770 - accuracy: 0.4300\n",
      "Epoch 4/5\n",
      "100/100 [==============================] - 0s 70us/step - loss: 1.0771 - accuracy: 0.4300\n",
      "Epoch 5/5\n",
      "100/100 [==============================] - 0s 90us/step - loss: 1.0759 - accuracy: 0.4300\n",
      "24/24 [==============================] - 0s 715us/step\n",
      "Mean validation MSE: 1.1514487266540527\n",
      "\n",
      "Now preprocessing hyper-parameter instance {'Flag': False, 'HiddenLayerActivationRelu': True}\n",
      "Epoch 1/5\n",
      "99/99 [==============================] - 0s 390us/step - loss: 1.1256 - accuracy: 0.4040\n",
      "Epoch 2/5\n",
      "99/99 [==============================] - 0s 60us/step - loss: 1.0582 - accuracy: 0.4444\n",
      "Epoch 3/5\n",
      "99/99 [==============================] - 0s 70us/step - loss: 0.9996 - accuracy: 0.5152\n",
      "Epoch 4/5\n",
      "99/99 [==============================] - 0s 60us/step - loss: 0.9506 - accuracy: 0.5657\n",
      "Epoch 5/5\n",
      "99/99 [==============================] - 0s 80us/step - loss: 0.9015 - accuracy: 0.6061\n",
      "25/25 [==============================] - 0s 621us/step\n",
      "Epoch 1/5\n",
      "99/99 [==============================] - 0s 424us/step - loss: 1.4813 - accuracy: 0.3636\n",
      "Epoch 2/5\n",
      "99/99 [==============================] - 0s 80us/step - loss: 1.3551 - accuracy: 0.4040\n",
      "Epoch 3/5\n",
      "99/99 [==============================] - 0s 71us/step - loss: 1.2429 - accuracy: 0.4545\n",
      "Epoch 4/5\n",
      "99/99 [==============================] - 0s 81us/step - loss: 1.1510 - accuracy: 0.4949\n",
      "Epoch 5/5\n",
      "99/99 [==============================] - 0s 70us/step - loss: 1.0688 - accuracy: 0.5354\n",
      "25/25 [==============================] - 0s 646us/step\n",
      "Epoch 1/5\n",
      "99/99 [==============================] - 0s 388us/step - loss: 1.3316 - accuracy: 0.4242\n",
      "Epoch 2/5\n",
      "99/99 [==============================] - 0s 71us/step - loss: 1.2398 - accuracy: 0.4444\n",
      "Epoch 3/5\n",
      "99/99 [==============================] - 0s 71us/step - loss: 1.1485 - accuracy: 0.4646\n",
      "Epoch 4/5\n",
      "99/99 [==============================] - 0s 91us/step - loss: 1.0758 - accuracy: 0.4848\n",
      "Epoch 5/5\n",
      "99/99 [==============================] - 0s 60us/step - loss: 1.0191 - accuracy: 0.5354\n",
      "25/25 [==============================] - 0s 645us/step\n",
      "Epoch 1/5\n",
      "99/99 [==============================] - 0s 444us/step - loss: 1.4097 - accuracy: 0.3838\n",
      "Epoch 2/5\n",
      "99/99 [==============================] - 0s 91us/step - loss: 1.3167 - accuracy: 0.4040\n",
      "Epoch 3/5\n",
      "99/99 [==============================] - 0s 70us/step - loss: 1.2373 - accuracy: 0.4141\n",
      "Epoch 4/5\n",
      "99/99 [==============================] - 0s 90us/step - loss: 1.1693 - accuracy: 0.4646\n",
      "Epoch 5/5\n",
      "99/99 [==============================] - 0s 90us/step - loss: 1.1058 - accuracy: 0.4747\n",
      "25/25 [==============================] - 0s 639us/step\n",
      "Epoch 1/5\n",
      "100/100 [==============================] - 0s 479us/step - loss: 1.4633 - accuracy: 0.5200\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 0s 89us/step - loss: 1.3201 - accuracy: 0.5700\n",
      "Epoch 3/5\n",
      "100/100 [==============================] - 0s 90us/step - loss: 1.1973 - accuracy: 0.5900\n",
      "Epoch 4/5\n",
      "100/100 [==============================] - 0s 90us/step - loss: 1.0910 - accuracy: 0.6100\n",
      "Epoch 5/5\n",
      "100/100 [==============================] - 0s 86us/step - loss: 1.0013 - accuracy: 0.6400\n",
      "24/24 [==============================] - 0s 705us/step\n",
      "Mean validation MSE: 0.8710221449534098\n",
      "\n",
      "Now preprocessing hyper-parameter instance {'Flag': False, 'HiddenLayerActivationRelu': False}\n",
      "Epoch 1/5\n",
      "99/99 [==============================] - 0s 398us/step - loss: 1.2492 - accuracy: 0.2525\n",
      "Epoch 2/5\n",
      "99/99 [==============================] - 0s 71us/step - loss: 1.2046 - accuracy: 0.2525\n",
      "Epoch 3/5\n",
      "99/99 [==============================] - 0s 61us/step - loss: 1.1662 - accuracy: 0.2525\n",
      "Epoch 4/5\n",
      "99/99 [==============================] - 0s 80us/step - loss: 1.1338 - accuracy: 0.2626\n",
      "Epoch 5/5\n",
      "99/99 [==============================] - 0s 50us/step - loss: 1.1050 - accuracy: 0.2626\n",
      "25/25 [==============================] - 0s 598us/step\n",
      "Epoch 1/5\n",
      "99/99 [==============================] - 0s 394us/step - loss: 1.4934 - accuracy: 0.3030\n",
      "Epoch 2/5\n",
      "99/99 [==============================] - 0s 70us/step - loss: 1.4151 - accuracy: 0.3030\n",
      "Epoch 3/5\n",
      "99/99 [==============================] - 0s 60us/step - loss: 1.3494 - accuracy: 0.3030\n",
      "Epoch 4/5\n",
      "99/99 [==============================] - 0s 90us/step - loss: 1.2814 - accuracy: 0.2929\n",
      "Epoch 5/5\n",
      "99/99 [==============================] - 0s 60us/step - loss: 1.2263 - accuracy: 0.2929\n",
      "25/25 [==============================] - 0s 659us/step\n",
      "Epoch 1/5\n",
      "99/99 [==============================] - 0s 435us/step - loss: 1.4090 - accuracy: 0.2828\n",
      "Epoch 2/5\n",
      "99/99 [==============================] - 0s 61us/step - loss: 1.3642 - accuracy: 0.2727\n",
      "Epoch 3/5\n",
      "99/99 [==============================] - 0s 70us/step - loss: 1.3155 - accuracy: 0.2828\n",
      "Epoch 4/5\n",
      "99/99 [==============================] - 0s 71us/step - loss: 1.2736 - accuracy: 0.3030\n",
      "Epoch 5/5\n",
      "99/99 [==============================] - 0s 70us/step - loss: 1.2349 - accuracy: 0.3434\n",
      "25/25 [==============================] - 0s 598us/step\n",
      "Epoch 1/5\n",
      "99/99 [==============================] - 0s 444us/step - loss: 1.1224 - accuracy: 0.1919\n",
      "Epoch 2/5\n",
      "99/99 [==============================] - 0s 70us/step - loss: 1.1050 - accuracy: 0.2424\n",
      "Epoch 3/5\n",
      "99/99 [==============================] - 0s 70us/step - loss: 1.0891 - accuracy: 0.2727\n",
      "Epoch 4/5\n",
      "99/99 [==============================] - 0s 60us/step - loss: 1.0731 - accuracy: 0.2929\n",
      "Epoch 5/5\n",
      "99/99 [==============================] - 0s 81us/step - loss: 1.0562 - accuracy: 0.3030\n",
      "25/25 [==============================] - 0s 678us/step\n",
      "Epoch 1/5\n",
      "100/100 [==============================] - 0s 390us/step - loss: 0.9159 - accuracy: 0.5100\n",
      "Epoch 2/5\n",
      "100/100 [==============================] - 0s 60us/step - loss: 0.9079 - accuracy: 0.5300\n",
      "Epoch 3/5\n",
      "100/100 [==============================] - 0s 60us/step - loss: 0.9010 - accuracy: 0.5400\n",
      "Epoch 4/5\n",
      "100/100 [==============================] - 0s 80us/step - loss: 0.8929 - accuracy: 0.5600\n",
      "Epoch 5/5\n",
      "100/100 [==============================] - 0s 60us/step - loss: 0.8859 - accuracy: 0.5700\n",
      "24/24 [==============================] - 0s 665us/step\n",
      "Mean validation MSE: 0.8928737640380859\n",
      "\n",
      "\n",
      "Best hyper-parameter instance: {'Flag': False, 'HiddenLayerActivationRelu': True}\n",
      "Epoch 1/5\n",
      "124/124 [==============================] - 0s 338us/step - loss: 1.2985 - accuracy: 0.3468\n",
      "Epoch 2/5\n",
      "124/124 [==============================] - 0s 64us/step - loss: 1.1620 - accuracy: 0.4113\n",
      "Epoch 3/5\n",
      "124/124 [==============================] - 0s 56us/step - loss: 1.0444 - accuracy: 0.4758\n",
      "Epoch 4/5\n",
      "124/124 [==============================] - 0s 72us/step - loss: 0.9464 - accuracy: 0.5323\n",
      "Epoch 5/5\n",
      "124/124 [==============================] - 0s 64us/step - loss: 0.8625 - accuracy: 0.6290\n",
      "54/54 [==============================] - 0s 295us/step\n",
      "Test MSE: 0.8413145630447952\n"
     ]
    }
   ],
   "source": [
    "# Number of folds in KFold cross-validation:\n",
    "num_folds = 5\n",
    "# Number of epochs:\n",
    "num_epochs = 5\n",
    "# Train batch size:\n",
    "train_batch_size = 16\n",
    "# Create the list of hyper-parameters instances:\n",
    "hyper_parameters_instances = [{\"Flag\": True, \"HiddenLayerActivationRelu\": True},\n",
    "                              {\"Flag\": True, \"HiddenLayerActivationRelu\": False},\n",
    "                              {\"Flag\": False, \"HiddenLayerActivationRelu\": True},\n",
    "                              {\"Flag\": False, \"HiddenLayerActivationRelu\": False}]\n",
    "# Also store the fixed hyper-parameters:\n",
    "fixed_hyper_parameters = {\"epochs\": num_epochs, \n",
    "                          \"train batch size\": train_batch_size}\n",
    "# Select model with KFold cross-validation:\n",
    "KFold_model_selection(X, y, fixed_hyper_parameters, hyper_parameters_instances, num_folds, seed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
