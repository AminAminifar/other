{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import seed\n",
    "from random import random\n",
    "from random import randrange\n",
    "import random\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split,KFold,cross_val_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, recall_score, precision_score\n",
    "from sklearn.utils.testing import ignore_warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import missingno as msno\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [20.0, 7.0]\n",
    "plt.rcParams.update({'font.size': 22,})\n",
    "sns.set_palette('viridis')\n",
    "sns.set_style('white')\n",
    "sns.set_context('talk', font_scale=0.8)\n",
    "warnings.filterwarnings(action='ignore', category=ConvergenceWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "# Create a random subsample from the dataset with replacement\n",
    "def subsample(X,y, ratio=0.8):\n",
    "    \n",
    "    # pick a random subsample of X and the corresponding y\n",
    "    num_of_instances = X.shape[0]\n",
    "    indices = np.random.randint(num_of_instances, size=round(num_of_instances*ratio))\n",
    "    sample_X, sample_y = X[indices,:], y[indices]\n",
    "    return sample_X, sample_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "# Bootstrap Aggregation Algorithm\n",
    "def bagging(X_train, y_train, X_test, n_clfs, Classifier):\n",
    "    clfs = list()\n",
    "    for i in range(n_clfs):\n",
    "        # train the clfs on the train subsamples with random_state = seed and add them to the list\n",
    "        Classifier.random_state = seed\n",
    "        features, labels = subsample(X_train,y_train)\n",
    "        clfs.append(Classifier().fit(features, labels))\n",
    "    \n",
    "    index = 0\n",
    "    y_ = [None] * X_test.shape[0]\n",
    "    for row in X_test:\n",
    "        row = row.reshape(1,-1)\n",
    "        # predict for each of the classifiers\n",
    "        predicted_y =  list()\n",
    "        for i in range(0, len(clfs)):\n",
    "            predicted_y.append(int(clfs[i].predict(row))) # int classes\n",
    "        #pick the prediction with the highest number\n",
    "        temp = np.argmax(np.bincount(predicted_y))\n",
    "        y_[index] = temp\n",
    "        index = index + 1\n",
    "                    \n",
    "    return(y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KFold_split(X, y, num_folds, seed):\n",
    "    KFold_splitter = KFold(n_splits=num_folds, shuffle=True, random_state=seed)\n",
    "    X_train_folds = []\n",
    "    X_val_folds = []\n",
    "    y_train_folds = []\n",
    "    y_val_folds = []\n",
    "    for (kth_fold_train_idxs, kth_fold_val_idxs) in KFold_splitter.split(X, y):\n",
    "        X_train_folds.append(X[kth_fold_train_idxs])\n",
    "        X_val_folds.append(X[kth_fold_val_idxs])\n",
    "        y_train_folds.append(y[kth_fold_train_idxs])\n",
    "        y_val_folds.append(y[kth_fold_val_idxs])\n",
    "    return X_train_folds, X_val_folds, y_train_folds, y_val_folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "def evaluate_algorithm(X_train_val, y_train_val, num_folds, seed, algorithm, *args):\n",
    "    # Extract train and validation folds:\n",
    "    X_train_folds, X_val_folds, y_train_folds, y_val_folds = KFold_split(X_train_val, y_train_val, num_folds, seed)\n",
    "    scores = list()\n",
    "    \n",
    "    for X_train_fold, X_val_fold, y_train_fold, y_val_fold in zip(X_train_folds, X_val_folds\n",
    "                                                                  , y_train_folds, y_val_folds):\n",
    "        predictions = algorithm(X_train_fold, y_train_fold, X_val_fold, *args)\n",
    "        scores.append(accuracy_score(y_val_fold, predictions))#compute the accuracy\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trees: 10\n",
      "Scores: [0.7241379310344828, 0.7931034482758621, 0.7241379310344828, 0.6896551724137931, 0.7586206896551724]\n",
      "Mean Accuracy: 0.738\n",
      "Trees: 50\n",
      "Scores: [0.8620689655172413, 0.7586206896551724, 0.8275862068965517, 0.7931034482758621, 0.7241379310344828]\n",
      "Mean Accuracy: 0.793\n",
      "Trees: 100\n",
      "Scores: [0.7586206896551724, 0.7931034482758621, 0.7586206896551724, 0.7931034482758621, 0.6896551724137931]\n",
      "Mean Accuracy: 0.759\n",
      "Trees: 150\n",
      "Scores: [0.7931034482758621, 0.7931034482758621, 0.8275862068965517, 0.8275862068965517, 0.6896551724137931]\n",
      "Mean Accuracy: 0.786\n",
      "Trees: 200\n",
      "Scores: [0.7586206896551724, 0.7586206896551724, 0.7586206896551724, 0.8275862068965517, 0.6896551724137931]\n",
      "Mean Accuracy: 0.759\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Test bagging on the sonar dataset\n",
    "seed = 2\n",
    "# load and prepare data\n",
    "filename = 'sonar.all-data'\n",
    "dataset = pd.read_csv(filename,header=None)\n",
    "X = dataset.iloc[:,:-1].to_numpy()\n",
    "y = (dataset.iloc[:,-1].to_numpy()=='M').astype(int)\n",
    "# evaluate algorithm\n",
    "num_folds = 5\n",
    "sample_size = 0.8\n",
    "random.seed(seed)\n",
    "# Extract a test set:\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.3, shuffle=True, random_state=seed)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# For each hyper-parameter instance, do KFold cross validation:\n",
    "for n_trees in [10, 50, 100, 150, 200]:\n",
    "    scores = evaluate_algorithm(X_train_val, y_train_val, num_folds, seed, bagging, n_trees, DecisionTreeClassifier) #compute scores for the n_trees\n",
    "    print('Trees: %d' % n_trees)\n",
    "    print('Scores: %s' % scores)\n",
    "    print('Mean Accuracy: %.3f' % (sum(scores)/float(len(scores))))\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set Accuracy: 0.794\n"
     ]
    }
   ],
   "source": [
    "Best_n = 50\n",
    "print('Test set Accuracy: %.3f' %(accuracy_score(bagging(X_train_val,y_train_val,X_test,Best_n\n",
    "                                                             ,DecisionTreeClassifier),y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Missing numerical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_diabetes():\n",
    "    dataset = pd.read_csv('pima-indians-diabetes.csv', header=None)\n",
    "    print(dataset.describe())\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                0           1           2           3           4           5  \\\n",
      "count  768.000000  768.000000  768.000000  768.000000  768.000000  768.000000   \n",
      "mean     3.845052  120.894531   69.105469   20.536458   79.799479   31.992578   \n",
      "std      3.369578   31.972618   19.355807   15.952218  115.244002    7.884160   \n",
      "min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
      "25%      1.000000   99.000000   62.000000    0.000000    0.000000   27.300000   \n",
      "50%      3.000000  117.000000   72.000000   23.000000   30.500000   32.000000   \n",
      "75%      6.000000  140.250000   80.000000   32.000000  127.250000   36.600000   \n",
      "max     17.000000  199.000000  122.000000   99.000000  846.000000   67.100000   \n",
      "\n",
      "                6           7           8  \n",
      "count  768.000000  768.000000  768.000000  \n",
      "mean     0.471876   33.240885    0.348958  \n",
      "std      0.331329   11.760232    0.476951  \n",
      "min      0.078000   21.000000    0.000000  \n",
      "25%      0.243750   24.000000    0.000000  \n",
      "50%      0.372500   29.000000    0.000000  \n",
      "75%      0.626250   41.000000    1.000000  \n",
      "max      2.420000   81.000000    1.000000  \n",
      "     0    1   2   3    4     5      6   7  8\n",
      "0    6  148  72  35    0  33.6  0.627  50  1\n",
      "1    1   85  66  29    0  26.6  0.351  31  0\n",
      "2    8  183  64   0    0  23.3  0.672  32  1\n",
      "3    1   89  66  23   94  28.1  0.167  21  0\n",
      "4    0  137  40  35  168  43.1  2.288  33  1\n",
      "5    5  116  74   0    0  25.6  0.201  30  0\n",
      "6    3   78  50  32   88  31.0  0.248  26  1\n",
      "7   10  115   0   0    0  35.3  0.134  29  0\n",
      "8    2  197  70  45  543  30.5  0.158  53  1\n",
      "9    8  125  96   0    0   0.0  0.232  54  1\n",
      "10   4  110  92   0    0  37.6  0.191  30  0\n",
      "11  10  168  74   0    0  38.0  0.537  34  1\n",
      "12  10  139  80   0    0  27.1  1.441  57  0\n",
      "13   1  189  60  23  846  30.1  0.398  59  1\n",
      "14   5  166  72  19  175  25.8  0.587  51  1\n",
      "15   7  100   0   0    0  30.0  0.484  32  1\n",
      "16   0  118  84  47  230  45.8  0.551  31  1\n",
      "17   7  107  74   0    0  29.6  0.254  31  1\n",
      "18   1  103  30  38   83  43.3  0.183  33  0\n",
      "19   1  115  70  30   96  34.6  0.529  32  1\n"
     ]
    }
   ],
   "source": [
    "# print the first 20 rows of data\n",
    "dataset = load_diabetes()\n",
    "print(dataset.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 35, 227, 374, 11]\n"
     ]
    }
   ],
   "source": [
    "#TODO\n",
    "#Count the number of zero values in column indeces [1,2,3,4,5]\n",
    "num_of_zeros = [None] * 5\n",
    "for i in [1,2,3,4,5]:\n",
    "    thelist = dataset.values[:,i].tolist()\n",
    "    frequencies = np.array(np.unique(thelist, return_counts=True)).T\n",
    "    index = np.where(frequencies[:,0] == 0)\n",
    "    if index[0].size != 0:\n",
    "        num_of_zeros[i-1] = int(frequencies[index[0],1])\n",
    "print(num_of_zeros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "C:\\Users\\Amin\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\core\\indexing.py:205: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      0\n",
      "1      5\n",
      "2     35\n",
      "3    227\n",
      "4    374\n",
      "5     11\n",
      "6      0\n",
      "7      0\n",
      "8      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#TODO\n",
    "# mark zero values as missing or NaN\n",
    "for i in [1,2,3,4,5]:\n",
    "    for j in range(dataset.values.shape[0]):\n",
    "        if dataset.values[j,i]==0:\n",
    "            dataset[i][j] = np.nan\n",
    "# count the number of NaN values in each column\n",
    "print(dataset.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>148.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>85.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>183.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>89.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>116.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25.6</td>\n",
       "      <td>0.201</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>78.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.248</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>115.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>35.3</td>\n",
       "      <td>0.134</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>197.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>543.0</td>\n",
       "      <td>30.5</td>\n",
       "      <td>0.158</td>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>125.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.232</td>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>110.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.6</td>\n",
       "      <td>0.191</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>168.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.537</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>139.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>27.1</td>\n",
       "      <td>1.441</td>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>189.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>846.0</td>\n",
       "      <td>30.1</td>\n",
       "      <td>0.398</td>\n",
       "      <td>59</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>166.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>25.8</td>\n",
       "      <td>0.587</td>\n",
       "      <td>51</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>7</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.484</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>230.0</td>\n",
       "      <td>45.8</td>\n",
       "      <td>0.551</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>7</td>\n",
       "      <td>107.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29.6</td>\n",
       "      <td>0.254</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>103.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>43.3</td>\n",
       "      <td>0.183</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>115.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>34.6</td>\n",
       "      <td>0.529</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0      1     2     3      4     5      6   7  8\n",
       "0    6  148.0  72.0  35.0    NaN  33.6  0.627  50  1\n",
       "1    1   85.0  66.0  29.0    NaN  26.6  0.351  31  0\n",
       "2    8  183.0  64.0   NaN    NaN  23.3  0.672  32  1\n",
       "3    1   89.0  66.0  23.0   94.0  28.1  0.167  21  0\n",
       "4    0  137.0  40.0  35.0  168.0  43.1  2.288  33  1\n",
       "5    5  116.0  74.0   NaN    NaN  25.6  0.201  30  0\n",
       "6    3   78.0  50.0  32.0   88.0  31.0  0.248  26  1\n",
       "7   10  115.0   NaN   NaN    NaN  35.3  0.134  29  0\n",
       "8    2  197.0  70.0  45.0  543.0  30.5  0.158  53  1\n",
       "9    8  125.0  96.0   NaN    NaN   NaN  0.232  54  1\n",
       "10   4  110.0  92.0   NaN    NaN  37.6  0.191  30  0\n",
       "11  10  168.0  74.0   NaN    NaN  38.0  0.537  34  1\n",
       "12  10  139.0  80.0   NaN    NaN  27.1  1.441  57  0\n",
       "13   1  189.0  60.0  23.0  846.0  30.1  0.398  59  1\n",
       "14   5  166.0  72.0  19.0  175.0  25.8  0.587  51  1\n",
       "15   7  100.0   NaN   NaN    NaN  30.0  0.484  32  1\n",
       "16   0  118.0  84.0  47.0  230.0  45.8  0.551  31  1\n",
       "17   7  107.0  74.0   NaN    NaN  29.6  0.254  31  1\n",
       "18   1  103.0  30.0  38.0   83.0  43.3  0.183  33  0\n",
       "19   1  115.0  70.0  30.0   96.0  34.6  0.529  32  1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(392, 9)\n"
     ]
    }
   ],
   "source": [
    "#TODO\n",
    "#delete rows contating NAN values from the dataset using .dropna(inplace = True) built-in function\n",
    "# print (dataset.shape)\n",
    "dataset.dropna(inplace=True)\n",
    "print (dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = dataset.values\n",
    "X = values[:,0:8]\n",
    "y = values[:,8]\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.25, shuffle=True, random_state=seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#TODO\n",
    "#complete the function to \n",
    "#evaluate the MLP model on the test set\n",
    "def evaluate_MLP(X_train, y_train,X_valid,y_valid,seed=7):\n",
    "    model = MLPClassifier()\n",
    "    model.fit(X_train, y_train)\n",
    "    result = model.predict(X_valid)\n",
    "    print(accuracy_score(result,y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.42857142857142855\n"
     ]
    }
   ],
   "source": [
    "evaluate_MLP(X_train, y_train,X_valid,y_valid,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                0           1           2           3           4           5  \\\n",
      "count  768.000000  768.000000  768.000000  768.000000  768.000000  768.000000   \n",
      "mean     3.845052  120.894531   69.105469   20.536458   79.799479   31.992578   \n",
      "std      3.369578   31.972618   19.355807   15.952218  115.244002    7.884160   \n",
      "min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
      "25%      1.000000   99.000000   62.000000    0.000000    0.000000   27.300000   \n",
      "50%      3.000000  117.000000   72.000000   23.000000   30.500000   32.000000   \n",
      "75%      6.000000  140.250000   80.000000   32.000000  127.250000   36.600000   \n",
      "max     17.000000  199.000000  122.000000   99.000000  846.000000   67.100000   \n",
      "\n",
      "                6           7           8  \n",
      "count  768.000000  768.000000  768.000000  \n",
      "mean     0.471876   33.240885    0.348958  \n",
      "std      0.331329   11.760232    0.476951  \n",
      "min      0.078000   21.000000    0.000000  \n",
      "25%      0.243750   24.000000    0.000000  \n",
      "50%      0.372500   29.000000    0.000000  \n",
      "75%      0.626250   41.000000    1.000000  \n",
      "max      2.420000   81.000000    1.000000  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  import sys\n",
      "C:\\Users\\Amin\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\core\\indexing.py:205: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0\n",
      "1    0\n",
      "2    0\n",
      "3    0\n",
      "4    0\n",
      "5    0\n",
      "6    0\n",
      "7    0\n",
      "8    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "dataset = load_diabetes()\n",
    "#TODO\n",
    "# Mark zero values of column indices [1,2,3,4,5] as missing or NaN\n",
    "for i in [1,2,3,4,5]:\n",
    "    for j in range(dataset.values.shape[0]):\n",
    "        if dataset.values[j,i]==0:\n",
    "            dataset[i][j] = np.nan\n",
    "# This time fill missing values with mean column values using .fillna(dataset.mean(),inplace=True)\n",
    "dataset.fillna(dataset.mean(),inplace=True)\n",
    "# count the number of NaN values in each column\n",
    "print(dataset.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>148.0</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>35.00000</td>\n",
       "      <td>155.548223</td>\n",
       "      <td>33.600000</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>85.0</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>29.00000</td>\n",
       "      <td>155.548223</td>\n",
       "      <td>26.600000</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>183.0</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>29.15342</td>\n",
       "      <td>155.548223</td>\n",
       "      <td>23.300000</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>89.0</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>23.00000</td>\n",
       "      <td>94.000000</td>\n",
       "      <td>28.100000</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>35.00000</td>\n",
       "      <td>168.000000</td>\n",
       "      <td>43.100000</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>116.0</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>29.15342</td>\n",
       "      <td>155.548223</td>\n",
       "      <td>25.600000</td>\n",
       "      <td>0.201</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>78.0</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>32.00000</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>0.248</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>115.0</td>\n",
       "      <td>72.405184</td>\n",
       "      <td>29.15342</td>\n",
       "      <td>155.548223</td>\n",
       "      <td>35.300000</td>\n",
       "      <td>0.134</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>197.0</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>45.00000</td>\n",
       "      <td>543.000000</td>\n",
       "      <td>30.500000</td>\n",
       "      <td>0.158</td>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>125.0</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>29.15342</td>\n",
       "      <td>155.548223</td>\n",
       "      <td>32.457464</td>\n",
       "      <td>0.232</td>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>110.0</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>29.15342</td>\n",
       "      <td>155.548223</td>\n",
       "      <td>37.600000</td>\n",
       "      <td>0.191</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>168.0</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>29.15342</td>\n",
       "      <td>155.548223</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>0.537</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>139.0</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>29.15342</td>\n",
       "      <td>155.548223</td>\n",
       "      <td>27.100000</td>\n",
       "      <td>1.441</td>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>189.0</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>23.00000</td>\n",
       "      <td>846.000000</td>\n",
       "      <td>30.100000</td>\n",
       "      <td>0.398</td>\n",
       "      <td>59</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>166.0</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>19.00000</td>\n",
       "      <td>175.000000</td>\n",
       "      <td>25.800000</td>\n",
       "      <td>0.587</td>\n",
       "      <td>51</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>7</td>\n",
       "      <td>100.0</td>\n",
       "      <td>72.405184</td>\n",
       "      <td>29.15342</td>\n",
       "      <td>155.548223</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>0.484</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>47.00000</td>\n",
       "      <td>230.000000</td>\n",
       "      <td>45.800000</td>\n",
       "      <td>0.551</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>7</td>\n",
       "      <td>107.0</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>29.15342</td>\n",
       "      <td>155.548223</td>\n",
       "      <td>29.600000</td>\n",
       "      <td>0.254</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>103.0</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>38.00000</td>\n",
       "      <td>83.000000</td>\n",
       "      <td>43.300000</td>\n",
       "      <td>0.183</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>115.0</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>30.00000</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>34.600000</td>\n",
       "      <td>0.529</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0      1          2         3           4          5      6   7  8\n",
       "0    6  148.0  72.000000  35.00000  155.548223  33.600000  0.627  50  1\n",
       "1    1   85.0  66.000000  29.00000  155.548223  26.600000  0.351  31  0\n",
       "2    8  183.0  64.000000  29.15342  155.548223  23.300000  0.672  32  1\n",
       "3    1   89.0  66.000000  23.00000   94.000000  28.100000  0.167  21  0\n",
       "4    0  137.0  40.000000  35.00000  168.000000  43.100000  2.288  33  1\n",
       "5    5  116.0  74.000000  29.15342  155.548223  25.600000  0.201  30  0\n",
       "6    3   78.0  50.000000  32.00000   88.000000  31.000000  0.248  26  1\n",
       "7   10  115.0  72.405184  29.15342  155.548223  35.300000  0.134  29  0\n",
       "8    2  197.0  70.000000  45.00000  543.000000  30.500000  0.158  53  1\n",
       "9    8  125.0  96.000000  29.15342  155.548223  32.457464  0.232  54  1\n",
       "10   4  110.0  92.000000  29.15342  155.548223  37.600000  0.191  30  0\n",
       "11  10  168.0  74.000000  29.15342  155.548223  38.000000  0.537  34  1\n",
       "12  10  139.0  80.000000  29.15342  155.548223  27.100000  1.441  57  0\n",
       "13   1  189.0  60.000000  23.00000  846.000000  30.100000  0.398  59  1\n",
       "14   5  166.0  72.000000  19.00000  175.000000  25.800000  0.587  51  1\n",
       "15   7  100.0  72.405184  29.15342  155.548223  30.000000  0.484  32  1\n",
       "16   0  118.0  84.000000  47.00000  230.000000  45.800000  0.551  31  1\n",
       "17   7  107.0  74.000000  29.15342  155.548223  29.600000  0.254  31  1\n",
       "18   1  103.0  30.000000  38.00000   83.000000  43.300000  0.183  33  0\n",
       "19   1  115.0  70.000000  30.00000   96.000000  34.600000  0.529  32  1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = dataset.values\n",
    "X = values[:,0:8]\n",
    "y = values[:,8]\n",
    "X_train, _, y_train, _ = train_test_split(X, y, test_size=0.25, shuffle=True, random_state=seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(576, 8)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6938775510204082\n"
     ]
    }
   ],
   "source": [
    "evaluate_MLP(X_train, y_train,X_valid,y_valid,7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Imbalanced Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(284807, 31)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
       "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
       "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
       "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
       "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
       "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
       "\n",
       "        V26       V27       V28  Amount  Class  \n",
       "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('creditcard.csv')\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    284315\n",
      "1       492\n",
      "Name: Class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.Class.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x504 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# using seaborns countplot to show distribution of questions in dataset\n",
    "fig, ax = plt.subplots()\n",
    "g = sns.countplot(df.Class, palette='viridis')\n",
    "g.set_xticklabels(['Not Fraud', 'Fraud'])\n",
    "g.set_yticklabels([])\n",
    "\n",
    "# function to show values on bars\n",
    "def show_values_on_bars(axs):\n",
    "    def _show_on_single_plot(ax):        \n",
    "        for p in ax.patches:\n",
    "            _x = p.get_x() + p.get_width() / 2\n",
    "            _y = p.get_y() + p.get_height()\n",
    "            value = '{:.0f}'.format(p.get_height())\n",
    "            ax.text(_x, _y, value, ha=\"center\") \n",
    "\n",
    "    if isinstance(axs, np.ndarray):\n",
    "        for idx, ax in np.ndenumerate(axs):\n",
    "            _show_on_single_plot(ax)\n",
    "    else:\n",
    "        _show_on_single_plot(axs)\n",
    "show_values_on_bars(ax)\n",
    "\n",
    "sns.despine(left=True, bottom=True)\n",
    "plt.xlabel('')\n",
    "plt.ylabel('')\n",
    "plt.title('Distribution of Transactions', fontsize=30)\n",
    "plt.tick_params(axis='x', which='major', labelsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0017304750013189597"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO\n",
    "# print percentage of samples where target == 1\n",
    "df.Class.value_counts()[1]/df.Class.value_counts()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for modeling\n",
    "# Separate input features and target\n",
    "y = df.Class\n",
    "X = df.drop('Class', axis=1)\n",
    "\n",
    "# setting up validation and training sets\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.25, random_state=27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique predicted labels:  [0 1]\n",
      "Validation score:  0.9965590854189489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\dummy.py:227: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  k in range(self.n_outputs_)).T\n"
     ]
    }
   ],
   "source": [
    "#TODO\n",
    "# Train a DummyClassifier to predict with 'most_frequent' strategy\n",
    "dummy = DummyClassifier()\n",
    "dummy.fit(X_train,y_train)\n",
    "dummy_pred = dummy.predict(X_valid)\n",
    "\n",
    "# checking unique labels\n",
    "print('Unique predicted labels: ', (np.unique(dummy_pred)))\n",
    "\n",
    "# checking accuracy\n",
    "print('Validation score: ', accuracy_score(y_valid, dummy_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_imbalanced(y_valid, lr_pred):\n",
    "    # Checking accuracy\n",
    "    print('Accuracy: ', accuracy_score(y_valid, lr_pred))\n",
    "    #recall score\n",
    "    print('Recall: ',recall_score(y_valid, lr_pred))\n",
    "    #precision score\n",
    "    print('Precision: ', precision_score(y_valid, lr_pred))\n",
    "    # f1 score\n",
    "    print('F1 score: ',f1_score(y_valid, lr_pred))\n",
    "    # confusion matrix\n",
    "    print('ConfMat')\n",
    "    print(pd.DataFrame(confusion_matrix(y_valid, lr_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "# Train a LogisticRegressio model with solver as 'liblinear' on the training data\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    " \n",
    "# Predict on validation set\n",
    "lr_pred = lr.predict(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9992135052386169\n",
      "Recall:  0.6439393939393939\n",
      "Precision:  0.9042553191489362\n",
      "F1 score:  0.7522123893805309\n",
      "ConfMat\n",
      "       0   1\n",
      "0  71061   9\n",
      "1     47  85\n"
     ]
    }
   ],
   "source": [
    "evaluate_imbalanced(y_valid, lr_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df.Class\n",
    "X = df.drop('Class', axis=1)\n",
    "\n",
    "# setting up validation and training sets\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.25, random_state=27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>264873</td>\n",
       "      <td>161634.0</td>\n",
       "      <td>-0.395578</td>\n",
       "      <td>1.489129</td>\n",
       "      <td>-0.833442</td>\n",
       "      <td>-0.224271</td>\n",
       "      <td>0.369444</td>\n",
       "      <td>-1.453886</td>\n",
       "      <td>0.796593</td>\n",
       "      <td>-0.060403</td>\n",
       "      <td>0.338270</td>\n",
       "      <td>...</td>\n",
       "      <td>0.231624</td>\n",
       "      <td>0.955194</td>\n",
       "      <td>-0.172092</td>\n",
       "      <td>-0.041050</td>\n",
       "      <td>-0.313444</td>\n",
       "      <td>-0.174301</td>\n",
       "      <td>0.064657</td>\n",
       "      <td>-0.036960</td>\n",
       "      <td>2.74</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>163821</td>\n",
       "      <td>116237.0</td>\n",
       "      <td>1.950487</td>\n",
       "      <td>0.002312</td>\n",
       "      <td>-1.761814</td>\n",
       "      <td>1.232470</td>\n",
       "      <td>0.523175</td>\n",
       "      <td>-0.650657</td>\n",
       "      <td>0.504231</td>\n",
       "      <td>-0.200857</td>\n",
       "      <td>0.116805</td>\n",
       "      <td>...</td>\n",
       "      <td>0.086306</td>\n",
       "      <td>0.326297</td>\n",
       "      <td>-0.068839</td>\n",
       "      <td>-0.416589</td>\n",
       "      <td>0.426044</td>\n",
       "      <td>-0.486299</td>\n",
       "      <td>-0.031266</td>\n",
       "      <td>-0.072543</td>\n",
       "      <td>38.44</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72083</td>\n",
       "      <td>54557.0</td>\n",
       "      <td>1.105167</td>\n",
       "      <td>-0.166253</td>\n",
       "      <td>0.569520</td>\n",
       "      <td>0.681043</td>\n",
       "      <td>-0.259189</td>\n",
       "      <td>0.642792</td>\n",
       "      <td>-0.437034</td>\n",
       "      <td>0.356746</td>\n",
       "      <td>0.441417</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009073</td>\n",
       "      <td>0.293023</td>\n",
       "      <td>-0.028688</td>\n",
       "      <td>-0.242206</td>\n",
       "      <td>0.389813</td>\n",
       "      <td>0.482852</td>\n",
       "      <td>0.010705</td>\n",
       "      <td>-0.008399</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>196949</td>\n",
       "      <td>131771.0</td>\n",
       "      <td>1.805238</td>\n",
       "      <td>0.961264</td>\n",
       "      <td>-1.717212</td>\n",
       "      <td>4.094625</td>\n",
       "      <td>0.938666</td>\n",
       "      <td>-0.227785</td>\n",
       "      <td>0.152911</td>\n",
       "      <td>0.066753</td>\n",
       "      <td>-1.073784</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.137875</td>\n",
       "      <td>-0.450959</td>\n",
       "      <td>0.098530</td>\n",
       "      <td>-0.662272</td>\n",
       "      <td>-0.150154</td>\n",
       "      <td>-0.098852</td>\n",
       "      <td>-0.000030</td>\n",
       "      <td>0.017622</td>\n",
       "      <td>37.89</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126213</td>\n",
       "      <td>77959.0</td>\n",
       "      <td>0.835421</td>\n",
       "      <td>-1.191847</td>\n",
       "      <td>0.578455</td>\n",
       "      <td>0.586101</td>\n",
       "      <td>-1.236663</td>\n",
       "      <td>0.194617</td>\n",
       "      <td>-0.532404</td>\n",
       "      <td>0.061561</td>\n",
       "      <td>-0.734344</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.072349</td>\n",
       "      <td>-0.109154</td>\n",
       "      <td>-0.308356</td>\n",
       "      <td>0.011968</td>\n",
       "      <td>0.461350</td>\n",
       "      <td>-0.244810</td>\n",
       "      <td>0.031845</td>\n",
       "      <td>0.060910</td>\n",
       "      <td>237.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Time        V1        V2        V3        V4        V5        V6  \\\n",
       "264873  161634.0 -0.395578  1.489129 -0.833442 -0.224271  0.369444 -1.453886   \n",
       "163821  116237.0  1.950487  0.002312 -1.761814  1.232470  0.523175 -0.650657   \n",
       "72083    54557.0  1.105167 -0.166253  0.569520  0.681043 -0.259189  0.642792   \n",
       "196949  131771.0  1.805238  0.961264 -1.717212  4.094625  0.938666 -0.227785   \n",
       "126213   77959.0  0.835421 -1.191847  0.578455  0.586101 -1.236663  0.194617   \n",
       "\n",
       "              V7        V8        V9  ...       V21       V22       V23  \\\n",
       "264873  0.796593 -0.060403  0.338270  ...  0.231624  0.955194 -0.172092   \n",
       "163821  0.504231 -0.200857  0.116805  ...  0.086306  0.326297 -0.068839   \n",
       "72083  -0.437034  0.356746  0.441417  ...  0.009073  0.293023 -0.028688   \n",
       "196949  0.152911  0.066753 -1.073784  ... -0.137875 -0.450959  0.098530   \n",
       "126213 -0.532404  0.061561 -0.734344  ... -0.072349 -0.109154 -0.308356   \n",
       "\n",
       "             V24       V25       V26       V27       V28  Amount  Class  \n",
       "264873 -0.041050 -0.313444 -0.174301  0.064657 -0.036960    2.74      0  \n",
       "163821 -0.416589  0.426044 -0.486299 -0.031266 -0.072543   38.44      0  \n",
       "72083  -0.242206  0.389813  0.482852  0.010705 -0.008399    1.00      0  \n",
       "196949 -0.662272 -0.150154 -0.098852 -0.000030  0.017622   37.89      0  \n",
       "126213  0.011968  0.461350 -0.244810  0.031845  0.060910  237.00      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# concatenate our training data back together\n",
    "X = pd.concat([X_train, y_train], axis=1)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    284315\n",
       "0    284315\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO\n",
    "# separate minority and majority classes\n",
    "not_fraud =  pd.DataFrame(df, index=np.where(df.values[:,-1] == 0)[0])\n",
    "fraud = pd.DataFrame(df, index=np.where(df.values[:,-1] == 1)[0])\n",
    "\n",
    "# upsample minority using resample and n_samples equal to the size of majority\n",
    "fraud_upsampled = resample(fraud,\n",
    "                          replace=True, # sample with replacement\n",
    "                          n_samples=not_fraud.shape[0], # match number in majority class\n",
    "                          random_state=27) # reproducible result\n",
    "\n",
    "\n",
    "# combine majority and upsampled minority\n",
    "upsampled = pd.concat([not_fraud, fraud_upsampled])\n",
    "\n",
    "# check new class counts\n",
    "upsampled.Class.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "# trying logistic regression again with the balanced dataset\n",
    "y = upsampled.Class\n",
    "X = upsampled.drop('Class', axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=27)\n",
    "# y_train = ...\n",
    "# X_train = ...\n",
    "\n",
    "# Train a logistic regression with solver 'liblinear' on the train data\n",
    "upsampled = LogisticRegression(solver='liblinear')\n",
    "upsampled.fit(X_train, y_train)\n",
    "# predict on the test data\n",
    "upsampled_pred = upsampled.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9379563584180982\n",
      "Recall:  0.9023495685899778\n",
      "Precision:  0.9716875491798317\n",
      "F1 score:  0.9357358320096761\n",
      "ConfMat\n",
      "       0      1\n",
      "0  69125   1871\n",
      "1   6949  64213\n"
     ]
    }
   ],
   "source": [
    "evaluate_imbalanced(y_test, upsampled_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    492\n",
       "0    492\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# still using our separated classes fraud and not_fraud from above\n",
    "# TODO\n",
    "# downsample majority\n",
    "not_fraud_downsampled = resample(not_fraud,\n",
    "                                replace = False, # sample without replacement\n",
    "                                n_samples = fraud.shape[0], # match minority n\n",
    "                                random_state = 27) # reproducible results\n",
    "\n",
    "# combine minority and downsampled majority\n",
    "downsampled = pd.concat([not_fraud_downsampled, fraud])\n",
    "\n",
    "# checking counts\n",
    "downsampled.Class.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# trying logistic regression again with the undersampled dataset\n",
    "y = downsampled.Class\n",
    "X = downsampled.drop('Class', axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=27)\n",
    "# X_train = ...\n",
    "# y_train = ...\n",
    "\n",
    "# Train a logistic regression with solver 'liblinear' on the train data\n",
    "undersampled = LogisticRegression(solver='liblinear')\n",
    "undersampled.fit(X_train, y_train)\n",
    "undersampled_pred = undersampled.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9512195121951219\n",
      "Recall:  0.8983050847457628\n",
      "Precision:  1.0\n",
      "F1 score:  0.9464285714285715\n",
      "ConfMat\n",
      "     0    1\n",
      "0  128    0\n",
      "1   12  106\n"
     ]
    }
   ],
   "source": [
    "evaluate_imbalanced(y_test, undersampled_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('creditcard.csv')\n",
    "y = df.Class\n",
    "X = df.drop('Class', axis=1)\n",
    "\n",
    "# setting up testing and training sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a random forest model\n",
    "rfc = RandomForestClassifier()\n",
    "rfc.fit(X_train, y_train)\n",
    "# predict on test set\n",
    "rfc_pred = rfc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9995786635206876\n",
      "Recall:  0.803030303030303\n",
      "Precision:  0.9636363636363636\n",
      "F1 score:  0.8760330578512396\n",
      "ConfMat\n",
      "       0    1\n",
      "0  71066    4\n",
      "1     26  106\n"
     ]
    }
   ],
   "source": [
    "evaluate_imbalanced(y_test,rfc_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
